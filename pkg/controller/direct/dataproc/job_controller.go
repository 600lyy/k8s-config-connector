// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// +tool:controller
// proto.service: google.cloud.dataproc.v1.JobController
// proto.message: google.cloud.dataproc.v1.Job
// crd.type: DataprocJob
// crd.version: v1alpha1

package dataproc

import (
	"context"
	"fmt"
	"strings"

	dataproc "cloud.google.com/go/dataproc/v2/apiv1"
	pb "cloud.google.com/go/dataproc/v2/apiv1/dataprocpb"
	"google.golang.org/protobuf/proto"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/klog/v2"
	"sigs.k8s.io/controller-runtime/pkg/client"

	krm "github.com/GoogleCloudPlatform/k8s-config-connector/apis/dataproc/v1alpha1"
	refs "github.com/GoogleCloudPlatform/k8s-config-connector/apis/refs/v1beta1"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/config"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct/directbase"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/controller/direct/registry"
	"github.com/GoogleCloudPlatform/k8s-config-connector/pkg/util"
)

const (
	// Dataproc jobs cannot be updated.
	// Ref: https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/patch
	// "Only the job state and labels are mutable."
	// State is managed by the service, and labels are metadata.
	// We consider the core job configuration immutable.
	dataprocJobErrCannotUpdate = "dataproc jobs cannot be updated"
)

func init() {
	registry.RegisterModel(krm.DataprocJobGVK, NewDataprocJobModel)
}

func NewDataprocJobModel(ctx context.Context, config *config.ControllerConfig) (directbase.Model, error) {
	return &dataprocJobModel{config: *config}, nil
}

var _ directbase.Model = &dataprocJobModel{}

type dataprocJobModel struct {
	config config.ControllerConfig
}

func (m *dataprocJobModel) client(ctx context.Context) (*dataproc.JobControllerClient, error) {
	opts, err := m.config.RESTClientOptions()
	if err != nil {
		return nil, err
	}

	return dataproc.NewJobControllerRESTClient(ctx, opts...)
}

func (m *dataprocJobModel) AdapterForObject(ctx context.Context, reader client.Reader, u *unstructured.Unstructured) (directbase.Adapter, error) {
	obj := &krm.DataprocJob{}
	if err := runtime.DefaultUnstructuredConverter.FromUnstructured(u.Object, &obj); err != nil {
		return nil, fmt.Errorf("error converting to %T: %w", obj, err)
	}

	projectID := obj.Spec.ProjectRef.External
	if projectID == "" {
		return nil, fmt.Errorf("projectRef.external is required")
	}
	location := obj.Spec.Region // Note: Dataproc API uses 'region', KCC uses 'location' sometimes, but here KRM uses 'Region'.
	if location == "" {
		return nil, fmt.Errorf("region is required")
	}

	// The job ID is part of the job placement config in the proto, not the top-level resource ID.
	// The job resource itself uses a UUID generated by the service (`reference.jobId`).
	// We construct an internal ID representation that includes the project and location.
	// When creating, we don't know the service-generated Job ID yet.
	var jobID *krm.DataprocJobID
	if obj.Status.Reference != nil && obj.Status.Reference.JobId != "" {
		jobID = &krm.DataprocJobID{
			ProjectID: projectID,
			Region:    location,
			JobID:     obj.Status.Reference.JobId,
		}
	} else {
		// For create/find purposes before creation, we only need project/location.
		// The actual job ID comes from the response.
		jobID = &krm.DataprocJobID{
			ProjectID: projectID,
			Region:    location,
			// JobID is empty initially
		}
	}

	mapCtx := &direct.MapContext{}
	desiredProto := DataprocJobSpec_ToProto(mapCtx, &obj.Spec)
	if mapCtx.Err() != nil {
		return nil, mapCtx.Err()
	}
	// We need the full Job message, not just the spec part.
	desired := &pb.Job{
		Placement:              desiredProto.Placement,
		JobUuid:                desiredProto.JobUuid,
		Scheduling:             desiredProto.Scheduling,
		YarnApplications:       desiredProto.YarnApplications,
		DriverSchedulingConfig: desiredProto.DriverSchedulingConfig,
		Done:                   desiredProto.Done,
		Labels:                 desiredProto.Labels,
	}
	// Populate the job type specific field
	switch obj.Spec.Type.(type) {
	case *krm.JobHadoopJob:
		desired.TypeJob = &pb.Job_HadoopJob{HadoopJob: desiredProto.GetHadoopJob()}
	case *krm.JobSparkJob:
		desired.TypeJob = &pb.Job_SparkJob{SparkJob: desiredProto.GetSparkJob()}
	case *krm.JobPysparkJob:
		desired.TypeJob = &pb.Job_PysparkJob{PysparkJob: desiredProto.GetPysparkJob()}
	case *krm.JobHiveJob:
		desired.TypeJob = &pb.Job_HiveJob{HiveJob: desiredProto.GetHiveJob()}
	case *krm.JobPigJob:
		desired.TypeJob = &pb.Job_PigJob{PigJob: desiredProto.GetPigJob()}
	case *krm.JobSparkRJob:
		desired.TypeJob = &pb.Job_SparkRJob{SparkRJob: desiredProto.GetSparkRJob()}
	case *krm.JobSparkSqlJob:
		desired.TypeJob = &pb.Job_SparkSqlJob{SparkSqlJob: desiredProto.GetSparkSqlJob()}
	case *krm.JobPrestoJob:
		desired.TypeJob = &pb.Job_PrestoJob{PrestoJob: desiredProto.GetPrestoJob()}
	case *krm.JobTrinoJob:
		desired.TypeJob = &pb.Job_TrinoJob{TrinoJob: desiredProto.GetTrinoJob()}
	default:
		return nil, fmt.Errorf("unknown job type specified")
	}

	gcpClient, err := m.client(ctx)
	if err != nil {
		return nil, err
	}

	return &dataprocJobAdapter{
		gcpClient: gcpClient,
		id:        jobID,
		desired:   desired,
		// resourceID is the user-specified name/id from metadata.name or resourceID annotation
		resourceID: direct.ValueOf(u.GetName()),
	}, nil
}

func (m *dataprocJobModel) AdapterForURL(ctx context.Context, url string) (directbase.Adapter, error) {
	// Expected format: //dataproc.googleapis.com/projects/<project>/regions/<region>/jobs/<jobId>
	if !strings.HasPrefix(url, "//dataproc.googleapis.com/") {
		return nil, nil
	}
	pattern := "projects/{project}/regions/{region}/jobs/{jobId}"
	tokens, err := direct.ParseURL(url, pattern)
	if err != nil {
		klog.V(4).Infof("url %q did not match pattern %q: %v", url, pattern, err)
		return nil, nil // Allow other models to handle
	}

	id := &krm.DataprocJobID{
		ProjectID: tokens["project"],
		Region:    tokens["region"],
		JobID:     tokens["jobId"],
	}

	gcpClient, err := m.client(ctx)
	if err != nil {
		return nil, err
	}

	return &dataprocJobAdapter{
		gcpClient: gcpClient,
		id:        id,
	}, nil
}

var _ directbase.Adapter = &dataprocJobAdapter{}

type dataprocJobAdapter struct {
	gcpClient  *dataproc.JobControllerClient
	id         *krm.DataprocJobID // Contains ProjectID, Region, and potentially JobID (after creation or from URL)
	desired    *pb.Job            // Desired state constructed from KRM spec
	actual     *pb.Job            // Actual state fetched from GCP
	resourceID string             // KRM resource name or resourceID annotation
}

func (a *dataprocJobAdapter) Find(ctx context.Context) (bool, error) {
	if a.id.JobID == "" {
		// If JobID is not known (e.g., before creation), we can't GET the job.
		// However, we might try to find it using a list call with a filter if
		// the user provided a stable ID via placement.jobId or resourceID annotation.
		// For now, assume we can only find by service-generated ID.
		// TODO(kcc): Implement List+Filter based on user-provided ID if available.
		klog.V(2).Infof("cannot find dataproc job without a job ID (project %s, region %s)", a.id.ProjectID, a.id.Region)
		return false, nil
	}

	req := &pb.GetJobRequest{
		ProjectId: a.id.ProjectID,
		Region:    a.id.Region,
		JobId:     a.id.JobID,
	}

	klog.V(2).Infof("getting dataproc job %q", a.id)
	actual, err := a.gcpClient.GetJob(ctx, req)
	if err != nil {
		if direct.IsNotFound(err) {
			klog.V(2).Infof("dataproc job %q not found", a.id)
			return false, nil
		}
		return false, fmt.Errorf("getting dataproc job %q: %w", a.id, err)
	}
	klog.V(2).Infof("found dataproc job %q", a.id)

	a.actual = actual

	// Check if the job is terminated (completed, cancelled, errored).
	// Once terminated, it's effectively immutable and shouldn't be reconciled further.
	// We treat it as "found" but signal it shouldn't be updated/deleted by KCC if it's done.
	// KCC deletion should still work if the user deletes the KRM object.
	// if a.actual.Status != nil && (a.actual.Status.State == pb.JobStatus_DONE || a.actual.Status.State == pb.JobStatus_CANCELLED || a.actual.Status.State == pb.JobStatus_ERROR) {
	// 	klog.Infof("Dataproc job %s/%s/%s is already in a terminal state (%s).", a.id.ProjectID, a.id.Region, a.id.JobID, a.actual.Status.State)
	// 	// TODO: How to signal this state back to the controller logic? Maybe via status?
	// }

	return true, nil
}

func (a *dataprocJobAdapter) Create(ctx context.Context, createOp *directbase.CreateOperation) error {
	klog.V(2).Infof("creating dataproc job in project %s region %s", a.id.ProjectID, a.id.Region)

	// Set the user-specified ID if provided in the spec.
	if a.desired.GetPlacement() != nil && a.desired.GetPlacement().GetJobUuid() != "" {
		// Placement.JobUuid is deprecated. Prefer setting reference.job_id
	} else if a.resourceID != "" {
		// If no specific job ID is in the spec, use the KRM resource name/resourceID
		// as the reference ID for submission. This makes jobs findable later if the
		// service-generated UUID is lost.
		if a.desired.Reference == nil {
			a.desired.Reference = &pb.JobReference{}
		}
		a.desired.Reference.JobId = a.resourceID
	}

	// Ensure placement is non-nil if it wasn't set, as it's often needed implicitly
	// even if empty (e.g., for cluster selectors). The mapping function should handle this.

	req := &pb.SubmitJobRequest{
		ProjectId: a.id.ProjectID,
		Region:    a.id.Region,
		Job:       a.desired,
		// RequestId: // Consider adding a UUID for idempotency?
	}

	submittedJob, err := a.gcpClient.SubmitJob(ctx, req)
	if err != nil {
		return fmt.Errorf("submitting dataproc job in %s/%s: %w", a.id.ProjectID, a.id.Region, err)
	}
	klog.V(2).Infof("successfully submitted dataproc job %s/%s, service generated ID: %s", a.id.ProjectID, a.id.Region, submittedJob.GetReference().GetJobId())

	// Update the ID with the service-generated ID for subsequent operations
	a.id.JobID = submittedJob.GetReference().GetJobId()
	a.actual = submittedJob // Store the immediate result as 'actual' state

	// Map the *returned* job state to the KRM status
	mapCtx := &direct.MapContext{}
	status := &krm.DataprocJobStatus{}
	status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, submittedJob)
	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}

	// Populate other status fields from the returned job
	status.Reference = DataprocJobStatusReference_FromProto(mapCtx, submittedJob.GetReference())
	status.Status = DataprocJobStatusStatus_FromProto(mapCtx, submittedJob.GetStatus())
	status.StatusHistory = DataprocJobStatusStatusHistory_FromProto(mapCtx, submittedJob.GetStatusHistory())
	status.YarnApplications = DataprocJobStatusYarnApplications_FromProto(mapCtx, submittedJob.GetYarnApplications())
	status.DriverOutputResourceUri = direct.LazyPtr(submittedJob.GetDriverOutputResourceUri())
	status.DriverControlFilesUri = direct.LazyPtr(submittedJob.GetDriverControlFilesUri())
	// ExternalRef needs the service-generated ID
	status.ExternalRef = direct.PtrTo(fmt.Sprintf("//dataproc.googleapis.com/projects/%s/regions/%s/jobs/%s", a.id.ProjectID, a.id.Region, a.id.JobID))

	if mapCtx.Err() != nil {
		return mapCtx.Err()
	}

	return createOp.UpdateStatus(ctx, status, nil)
}

func (a *dataprocJobAdapter) Update(ctx context.Context, updateOp *directbase.UpdateOperation) error {
	// Dataproc jobs are generally immutable after submission, except for labels.
	// Ref: https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/patch
	// We prevent updates to avoid unexpected behavior or errors.
	klog.Warningf("dataproc job %q: updates are not supported", a.id)

	// Check if only labels changed
	onlyLabelsChanged := true
	if a.actual == nil {
		// Should not happen in Update path, but defensively return error
		return fmt.Errorf("actual state is nil during update for job %q", a.id)
	}

	// Create copies for comparison, normalizing potential output-only fields
	desiredComparable := proto.Clone(a.desired).(*pb.Job)
	actualComparable := proto.Clone(a.actual).(*pb.Job)

	// Clear fields that are not part of the spec or are output-only before comparison
	clearOutputOnlyJobFields(desiredComparable)
	clearOutputOnlyJobFields(actualComparable)

	// Compare everything except labels
	desiredLabels := desiredComparable.Labels
	actualLabels := actualComparable.Labels
	desiredComparable.Labels = nil
	actualComparable.Labels = nil

	if !proto.Equal(desiredComparable, actualComparable) {
		onlyLabelsChanged = false
	}

	// Restore labels for potential label update
	desiredComparable.Labels = desiredLabels
	actualComparable.Labels = actualLabels

	if !onlyLabelsChanged {
		updateOp.Warn(dataprocJobErrCannotUpdate)
		// We still need to update the status with the observed state from Find()
		status := &krm.DataprocJobStatus{}
		mapCtx := &direct.MapContext{}
		status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, a.actual)
		status.Reference = DataprocJobStatusReference_FromProto(mapCtx, a.actual.GetReference())
		status.Status = DataprocJobStatusStatus_FromProto(mapCtx, a.actual.GetStatus())
		status.StatusHistory = DataprocJobStatusStatusHistory_FromProto(mapCtx, a.actual.GetStatusHistory())
		status.YarnApplications = DataprocJobStatusYarnApplications_FromProto(mapCtx, a.actual.GetYarnApplications())
		status.DriverOutputResourceUri = direct.LazyPtr(a.actual.GetDriverOutputResourceUri())
		status.DriverControlFilesUri = direct.LazyPtr(a.actual.GetDriverControlFilesUri())
		status.ExternalRef = direct.PtrTo(fmt.Sprintf("//dataproc.googleapis.com/projects/%s/regions/%s/jobs/%s", a.id.ProjectID, a.id.Region, a.id.JobID))
		if mapCtx.Err() != nil {
			return mapCtx.Err()
		}
		return updateOp.UpdateStatus(ctx, status, nil)
	}

	// If only labels changed, proceed with label update
	if !util.MapsEqual(a.desired.Labels, a.actual.Labels) {
		klog.V(2).Infof("updating labels for dataproc job %q", a.id)
		req := &pb.UpdateJobRequest{
			ProjectId: a.id.ProjectID,
			Region:    a.id.Region,
			JobId:     a.id.JobID,
			Job: &pb.Job{
				Labels: a.desired.Labels,
			},
			UpdateMask: &pb.FieldMask{Paths: []string{"labels"}},
		}
		updatedJob, err := a.gcpClient.UpdateJob(ctx, req)
		if err != nil {
			return fmt.Errorf("updating labels for dataproc job %q: %w", a.id, err)
		}
		klog.V(2).Infof("successfully updated labels for dataproc job %q", a.id)
		a.actual = updatedJob // Update actual state

		// Map the updated job state to the KRM status
		mapCtx := &direct.MapContext{}
		status := &krm.DataprocJobStatus{}
		status.ObservedState = DataprocJobObservedState_FromProto(mapCtx, updatedJob)
		status.Reference = DataprocJobStatusReference_FromProto(mapCtx, updatedJob.GetReference())
		status.Status = DataprocJobStatusStatus_FromProto(mapCtx, updatedJob.GetStatus())
		status.StatusHistory = DataprocJobStatusStatusHistory_FromProto(mapCtx, updatedJob.GetStatusHistory())
		status.YarnApplications = DataprocJobStatusYarnApplications_FromProto(mapCtx, updatedJob.GetYarnApplications())
		status.DriverOutputResourceUri = direct.LazyPtr(updatedJob.GetDriverOutputResourceUri())
		status.DriverControlFilesUri = direct.LazyPtr(updatedJob.GetDriverControlFilesUri())
		status.ExternalRef = direct.PtrTo(fmt.Sprintf("//dataproc.googleapis.com/projects/%s/regions/%s/jobs/%s", a.id.ProjectID, a.id.Region, a.id.JobID))
		if mapCtx.Err() != nil {
			return mapCtx.Err()
		}
		return updateOp.UpdateStatus(ctx, status, nil)
	}

	klog.V(2).Infof("no update needed for dataproc job %q", a.id)
	return nil // No changes detected
}

// clearOutputOnlyJobFields removes fields from a Job proto that are output-only
// or managed by the service, so they don't interfere with comparisons.
func clearOutputOnlyJobFields(job *pb.Job) {
	if job == nil {
		return
	}
	job.Status = nil
	job.StatusHistory = nil
	job.DriverOutputResourceUri = ""
	job.DriverControlFilesUri = ""
	job.JobUuid = ""           // Service-generated UUID
	job.Reference = nil        // Contains service-generated ID
	job.YarnApplications = nil // Runtime info
	job.Done = false           // Runtime status
	// Placement might contain output info like cluster UUID if not specified by name
	if job.Placement != nil {
		job.Placement.ClusterUuid = ""
	}
}

func (a *dataprocJobAdapter) Export(ctx context.Context) (*unstructured.Unstructured, error) {
	if a.actual == nil {
		return nil, fmt.Errorf("actual state is nil, cannot export")
	}

	mapCtx := &direct.MapContext{}
	spec := DataprocJobSpec_FromProto(mapCtx, a.actual)
	if mapCtx.Err() != nil {
		return nil, mapCtx.Err()
	}

	obj := &krm.DataprocJob{
		Spec: *spec,
	}
	obj.Spec.ProjectRef = refs.ProjectRef{External: a.id.ProjectID}
	obj.Spec.Region = a.id.Region

	// Set the correct job type wrapper in the spec
	switch jobType := a.actual.TypeJob.(type) {
	case *pb.Job_HadoopJob:
		obj.Spec.Type = &krm.JobHadoopJob{HadoopJob: spec.HadoopJob}
		obj.Spec.HadoopJob = nil // Clear the temporary holder
	case *pb.Job_SparkJob:
		obj.Spec.Type = &krm.JobSparkJob{SparkJob: spec.SparkJob}
		obj.Spec.SparkJob = nil
	case *pb.Job_PysparkJob:
		obj.Spec.Type = &krm.JobPysparkJob{PysparkJob: spec.PysparkJob}
		obj.Spec.PysparkJob = nil
	case *pb.Job_HiveJob:
		obj.Spec.Type = &krm.JobHiveJob{HiveJob: spec.HiveJob}
		obj.Spec.HiveJob = nil
	case *pb.Job_PigJob:
		obj.Spec.Type = &krm.JobPigJob{PigJob: spec.PigJob}
		obj.Spec.PigJob = nil
	case *pb.Job_SparkRJob:
		obj.Spec.Type = &krm.JobSparkRJob{SparkRJob: spec.SparkRJob}
		obj.Spec.SparkRJob = nil
	case *pb.Job_SparkSqlJob:
		obj.Spec.Type = &krm.JobSparkSqlJob{SparkSqlJob: spec.SparkSqlJob}
		obj.Spec.SparkSqlJob = nil
	case *pb.Job_PrestoJob:
		obj.Spec.Type = &krm.JobPrestoJob{PrestoJob: spec.PrestoJob}
		obj.Spec.PrestoJob = nil
	case *pb.Job_TrinoJob:
		obj.Spec.Type = &krm.JobTrinoJob{TrinoJob: spec.TrinoJob}
		obj.Spec.TrinoJob = nil
	default:
		// This should ideally not happen if the job was fetched correctly
		klog.Warningf("unknown job type found during export: %T", jobType)
	}

	uObj, err := runtime.DefaultUnstructuredConverter.ToUnstructured(obj)
	if err != nil {
		return nil, fmt.Errorf("converting DataprocJob to unstructured: %w", err)
	}

	u := &unstructured.Unstructured{Object: uObj}
	u.SetGroupVersionKind(krm.DataprocJobGVK)

	// Use the original reference job ID (if provided) or the service-generated ID as the name.
	// Fallback to a generated name if needed, though unlikely.
	name := a.actual.GetReference().GetJobId()
	if name == "" {
		// This should not happen for a fetched job
		name = "dataproc-job-" + a.id.JobID
	}
	u.SetName(name)

	klog.V(2).Infof("exported dataproc job %q as KRM object", a.id)
	return u, nil
}

func (a *dataprocJobAdapter) Delete(ctx context.Context, deleteOp *directbase.DeleteOperation) (bool, error) {
	// Dataproc jobs can be cancelled, which stops them. Deleting removes the record.
	// We will perform DeleteJob. If the job is terminal, Delete might be idempotent or unnecessary.
	// If the job is active, cancellation might be preferred, but KCC delete means remove the resource.
	if a.id.JobID == "" {
		// Cannot delete if we don't have the ID. This might mean the job was never created successfully.
		klog.Warningf("cannot delete dataproc job in %s/%s, job ID unknown", a.id.ProjectID, a.id.Region)
		return true, nil // Assume already gone
	}

	// Check if the job is already in a terminal state. If so, Delete might already be a no-op or unnecessary.
	// However, the API might still allow deleting the record. We proceed with delete regardless.
	if a.actual != nil && a.actual.Status != nil {
		state := a.actual.Status.State
		if state == pb.JobStatus_DONE || state == pb.JobStatus_CANCELLED || state == pb.JobStatus_ERROR {
			klog.V(2).Infof("dataproc job %q is already in terminal state %s, proceeding with deletion", a.id, state)
		}
	}

	klog.V(2).Infof("deleting dataproc job %q", a.id)
	req := &pb.DeleteJobRequest{
		ProjectId: a.id.ProjectID,
		Region:    a.id.Region,
		JobId:     a.id.JobID,
	}

	_, err := a.gcpClient.DeleteJob(ctx, req)
	if err != nil {
		if direct.IsNotFound(err) {
			klog.V(2).Infof("dataproc job %q already deleted", a.id)
			return true, nil // Already deleted
		}
		// Check for specific errors, e.g., if cancellation is needed first?
		// The API doc doesn't specify preconditions like needing to cancel first.
		return false, fmt.Errorf("deleting dataproc job %q: %w", a.id, err)
	}

	klog.V(2).Infof("successfully deleted dataproc job %q", a.id)
	return true, nil
}
