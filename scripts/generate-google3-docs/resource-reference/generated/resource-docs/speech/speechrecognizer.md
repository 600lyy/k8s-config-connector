{# AUTOGENERATED. DO NOT EDIT. #}

{% extends "config-connector/_base.html" %}

{% block page_title %}SpeechRecognizer{% endblock %}
{% block body %}



A brief introduction to SpeechRecognizer.

<table>
<thead>
<tr>
<th><strong>Property</strong></th>
<th><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>{{gcp_name_short}} Service Name</td>
<td>Speech-to-Text</td>
</tr>
<tr>
<td>{{gcp_name_short}} Service Documentation</td>
<td><a href="/speech-to-text/docs/">/speech-to-text/docs/</a></td>
</tr>
<tr>
<td>{{gcp_name_short}} REST Resource Name</td>
<td>v2.projects.locations.recognizers</td>
</tr>
<tr>
<td>{{gcp_name_short}} REST Resource Documentation</td>
<td><a href="/speech-to-text/docs/reference/rest/v2/projects.locations.recognizers">/speech-to-text/docs/reference/rest/v2/projects.locations.recognizers</a></td>
</tr>
<tr>
<td>{{product_name_short}} Resource Short Names</td>
<td>gcpspeechrecognizer<br>gcpspeechrecognizers<br>speechrecognizer</td>
</tr>
<tr>
<td>{{product_name_short}} Service Name</td>
<td>speech.googleapis.com</td>
</tr>
<tr>
<td>{{product_name_short}} Resource Fully Qualified Name</td>
<td>speechrecognizers.speech.cnrm.cloud.google.com</td>
</tr>

<tr>
    <td>Can Be Referenced by IAMPolicy/IAMPolicyMember</td>
    <td>No</td>
</tr>


<tr>
<td>{{product_name_short}} Default Average Reconcile Interval In Seconds</td>
<td>600</td>
</tr>
</tbody>
</table>

## Custom Resource Definition Properties



### Spec
#### Schema
```yaml
annotations:
  string: string
defaultRecognitionConfig:
  adaptation:
    customClasses:
    - annotations:
        string: string
      displayName: string
      items:
      - value: string
    phraseSets:
    - inlinePhraseSet:
        annotations:
          string: string
        boost: string
        displayName: string
        phrases:
        - boost: string
          value: string
      phraseSetRef:
        external: string
        name: string
        namespace: string
  autoDecodingConfig: {}
  explicitDecodingConfig:
    audioChannelCount: integer
    encoding: string
    sampleRateHertz: integer
  features:
    diarizationConfig:
      maxSpeakerCount: integer
      minSpeakerCount: integer
    enableAutomaticPunctuation: boolean
    enableSpokenEmojis: boolean
    enableSpokenPunctuation: boolean
    enableWordConfidence: boolean
    enableWordTimeOffsets: boolean
    maxAlternatives: integer
    multiChannelMode: string
    profanityFilter: boolean
  languageCodes:
  - string
  model: string
  transcriptNormalization:
    entries:
    - caseSensitive: boolean
      replace: string
      search: string
  translationConfig:
    targetLanguage: string
displayName: string
languageCodes:
- string
location: string
model: string
projectRef:
  external: string
  kind: string
  name: string
  namespace: string
resourceID: string
```

<table class="properties responsive">
<thead>
    <tr>
        <th colspan="2">Fields</th>
    </tr>
</thead>
<tbody>
    <tr>
        <td>
            <p><code>annotations</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Allows users to store small amounts of arbitrary data. Both the key and the value must be 63 characters or less each. At most 100 annotations.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Default configuration to use for requests with this Recognizer. This can be overwritten by inline configuration in the [RecognizeRequest.config][google.cloud.speech.v2.RecognizeRequest.config] field.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Speech adaptation context that weights recognizer predictions for specific words and phrases.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of inline CustomClasses. Existing CustomClass resources can be referenced directly in a PhraseSet.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[].annotations</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Optional. Allows users to store small amounts of arbitrary data. Both the key and the value must be 63 characters or less each. At most 100 annotations.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[].displayName</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. User-settable, human-readable name for the CustomClass. Must be 63 characters or less.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[].items</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A collection of class items.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[].items[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.customClasses[].items[].value</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The class item's value.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of inline or referenced PhraseSets.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}An inline defined PhraseSet.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.annotations</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">map (key: string, value: string)</code></p>
            <p>{% verbatim %}Allows users to store small amounts of arbitrary data. Both the key and the value must be 63 characters or less each. At most 100 annotations.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.boost</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Hint Boost. Positive value will increase the probability that a specific phrase will be recognized over other similar sounding phrases. The higher the boost, the higher the chance of false positive recognition as well. Valid `boost` values are between 0 (exclusive) and 20. We recommend using a binary search approach to finding the optimal value for your use case as well as adding phrases both with and without boost to your requests.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.displayName</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}User-settable, human-readable name for the PhraseSet. Must be 63 characters or less.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.phrases</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of word and phrases.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.phrases[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.phrases[].boost</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Hint Boost. Overrides the boost set at the phrase set level. Positive value will increase the probability that a specific phrase will be recognized over other similar sounding phrases. The higher the boost, the higher the chance of false positive recognition as well. Negative boost values would correspond to anti-biasing. Anti-biasing is not enabled, so negative boost values will return an error. Boost values must be between 0 and 20. Any values outside that range will return an error. We recommend using a binary search approach to finding the optimal value for your use case as well as adding phrases both with and without boost to your requests.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.phrases[].value</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The phrase itself.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].phraseSetRef</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}The name of an existing PhraseSet resource. The user must have read access to the resource and it must not be deleted.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].phraseSetRef.external</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}A reference to an externally managed SpeechPhraseSet resource. Should be in the format "projects/{{projectID}}/locations/{{location}}/phraseSets/{{phrasesetID}}".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].phraseSetRef.name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The name of a SpeechPhraseSet resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.adaptation.phraseSets[].phraseSetRef.namespace</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The namespace of a SpeechPhraseSet resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.autoDecodingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Automatically detect decoding parameters. Preferred for supported formats.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.explicitDecodingConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Explicitly specified decoding parameters. Required if using headerless PCM audio (linear16, mulaw, alaw).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.explicitDecodingConfig.audioChannelCount</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Number of channels present in the audio data sent for
 recognition. Note that this field is marked as OPTIONAL for backward
 compatibility reasons. It is (and has always been) effectively REQUIRED.

 The maximum allowed value is 8.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.explicitDecodingConfig.encoding</code></p>
            <p><i>Required*</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. Encoding of the audio data sent for recognition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.explicitDecodingConfig.sampleRateHertz</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Optional. Sample rate in Hertz of the audio data sent for recognition. Valid values are: 8000-48000. 16000 is optimal. For best results, set the sampling rate of the audio source to 16000 Hz. If that's not possible, use the native sample rate of the audio source (instead of re-sampling). Note that this field is marked as OPTIONAL for backward compatibility reasons. It is (and has always been) effectively REQUIRED.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Speech recognition features to enable.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.diarizationConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Configuration to enable speaker diarization and set additional parameters to make diarization better suited for your application. When this is enabled, we send all the words from the beginning of the audio for the top alternative in every consecutive STREAMING responses. This is done in order to improve our speaker tags as our models learn to identify the speakers in the conversation over time. For non-streaming requests, the diarization results will be provided only in the top alternative of the FINAL SpeechRecognitionResult.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.diarizationConfig.maxSpeakerCount</code></p>
            <p><i>Required*</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Required. Maximum number of speakers in the conversation. Valid values are: 1-6. Must be >= `min_speaker_count`. This range gives you more flexibility by allowing the system to automatically determine the correct number of speakers.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.diarizationConfig.minSpeakerCount</code></p>
            <p><i>Required*</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Required. Minimum number of speakers in the conversation. This range gives
 you more flexibility by allowing the system to automatically determine the
 correct number of speakers.

 To fix the number of speakers detected in the audio, set
 `min_speaker_count` = `max_speaker_count`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.enableAutomaticPunctuation</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}If `true`, adds punctuation to recognition result hypotheses. This feature is only available in select languages. The default `false` value does not add punctuation to result hypotheses.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.enableSpokenEmojis</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}The spoken emoji behavior for the call. If `true`, adds spoken emoji formatting for the request. This will replace spoken emojis with the corresponding Unicode symbols in the final transcript. If `false`, spoken emojis are not replaced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.enableSpokenPunctuation</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}The spoken punctuation behavior for the call. If `true`, replaces spoken punctuation with the corresponding symbols in the request. For example, "how are you question mark" becomes "how are you?". See https://cloud.google.com/speech-to-text/docs/spoken-punctuation for support. If `false`, spoken punctuation is not replaced.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.enableWordConfidence</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}If `true`, the top result includes a list of words and the confidence for those words. If `false`, no word-level confidence information is returned. The default is `false`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.enableWordTimeOffsets</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}If `true`, the top result includes a list of words and the start and end time offsets (timestamps) for those words. If `false`, no word-level time offset information is returned. The default is `false`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.maxAlternatives</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}Maximum number of recognition hypotheses to be returned. The server may return fewer than `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of one. If omitted, will return a maximum of one.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.multiChannelMode</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Mode for recognizing multi-channel audio.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.features.profanityFilter</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}If set to `true`, the server will attempt to filter out profanities, replacing all but the initial character in each filtered word with asterisks, for instance, "f***". If set to `false` or omitted, profanities won't be filtered out.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.languageCodes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. The language of the supplied audio as a
 [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
 Language tags are normalized to BCP-47 before they are used eg "en-us"
 becomes "en-US".

 Supported languages for each model are listed in the [Table of Supported
 Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).

 If additional languages are provided, recognition result will contain
 recognition in the most likely language detected. The recognition result
 will include the language tag of the language detected in the audio.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.languageCodes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.model</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. Which model to use for recognition requests. Select the model
 best suited to your domain to get best results.

 Guidance for choosing which model to use can be found in the [Transcription
 Models
 Documentation](https://cloud.google.com/speech-to-text/v2/docs/transcription-model)
 and the models supported in each region can be found in the [Table Of
 Supported
 Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Use transcription normalization to automatically replace parts of the transcript with phrases of your choosing. For StreamingRecognize, this normalization only applies to stable partial transcripts (stability > 0.8) and final transcripts.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization.entries</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of replacement entries. We will perform replacement with one entry at a time. For example, the second entry in ["cat" => "dog", "mountain cat" => "mountain dog"] will never be applied because we will always process the first entry before it. At most 100 entries.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization.entries[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization.entries[].caseSensitive</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Whether the search is case sensitive.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization.entries[].replace</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}What to replace with. Max length is 100 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.transcriptNormalization.entries[].search</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}What to replace. Max length is 100 characters.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.translationConfig</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Optional. Optional configuration used to automatically run translation on the given audio to the desired language for supported models.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>defaultRecognitionConfig.translationConfig.targetLanguage</code></p>
            <p><i>Required*</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Required. The language code to translate to.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>displayName</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}User-settable, human-readable name for the Recognizer. Must be 63 characters or less.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>languageCodes</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">list (string)</code></p>
            <p>{% verbatim %}Optional. This field is now deprecated. Prefer the
 [`language_codes`][google.cloud.speech.v2.RecognitionConfig.language_codes]
 field in the
 [`RecognitionConfig`][google.cloud.speech.v2.RecognitionConfig] message.

 The language of the supplied audio as a
 [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.

 Supported languages for each model are listed in the [Table of Supported
 Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).

 If additional languages are provided, recognition result will contain
 recognition in the most likely language detected. The recognition result
 will include the language tag of the language detected in the audio.
 When you create or update a Recognizer, these values are
 stored in normalized BCP-47 form. For example, "en-us" is stored as
 "en-US".{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>languageCodes[]</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>location</code></p>
            <p><i>Required</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Immutable.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>model</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Optional. This field is now deprecated. Prefer the
 [`model`][google.cloud.speech.v2.RecognitionConfig.model] field in the
 [`RecognitionConfig`][google.cloud.speech.v2.RecognitionConfig] message.

 Which model to use for recognition requests. Select the model best suited
 to your domain to get best results.

 Guidance for choosing which model to use can be found in the [Transcription
 Models
 Documentation](https://cloud.google.com/speech-to-text/v2/docs/transcription-model)
 and the models supported in each region can be found in the [Table Of
 Supported
 Models](https://cloud.google.com/speech-to-text/v2/docs/speech-to-text-supported-languages).{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef</code></p>
            <p><i>Required</i></p>
        </td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}The Project that this resource belongs to.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.external</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `projectID` field of a project, when not managed by Config Connector.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.kind</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The kind of the Project resource; optional but must be `Project` if provided.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.name</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `name` field of a `Project` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>projectRef.namespace</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The `namespace` field of a `Project` resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td>
            <p><code>resourceID</code></p>
            <p><i>Optional</i></p>
        </td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}The SpeechRecognizer name. If not given, the metadata.name will be used.{% endverbatim %}</p>
        </td>
    </tr>
</tbody>
</table>


<p>* Field is required when parent field is specified</p>


### Status
#### Schema
```yaml
conditions:
- lastTransitionTime: string
  message: string
  reason: string
  status: string
  type: string
externalRef: string
observedGeneration: integer
observedState:
  createTime: string
  defaultRecognitionConfig:
    adaptation:
      customClasses:
      - createTime: string
        deleteTime: string
        etag: string
        expireTime: string
        kmsKeyName: string
        kmsKeyVersionName: string
        name: string
        reconciling: boolean
        state: string
        uid: string
        updateTime: string
      phraseSets:
      - inlinePhraseSet:
          createTime: string
          deleteTime: string
          etag: string
          expireTime: string
          kmsKeyName: string
          kmsKeyVersionName: string
          name: string
          reconciling: boolean
          state: string
          uid: string
          updateTime: string
  deleteTime: string
  etag: string
  expireTime: string
  kmsKeyName: string
  kmsKeyVersionName: string
  reconciling: boolean
  state: string
  uid: string
  updateTime: string
```

<table class="properties responsive">
<thead>
    <tr>
        <th colspan="2">Fields</th>
    </tr>
</thead>
<tbody>
    <tr>
        <td><code>conditions</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}Conditions represent the latest available observations of the object's current state.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].lastTransitionTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Last time the condition transitioned from one status to another.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].message</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Human-readable message indicating details about last transition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].reason</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Unique, one-word, CamelCase reason for the condition's last transition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].status</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Status is the status of the condition. Can be True, False, Unknown.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>conditions[].type</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Type is the type of the condition.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>externalRef</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}A unique specifier for the SpeechRecognizer resource in GCP.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedGeneration</code></td>
        <td>
            <p><code class="apitype">integer</code></p>
            <p>{% verbatim %}ObservedGeneration is the generation of the resource that was most recently observed by the Config Connector controller. If this is equal to metadata.generation, then that means that the current reported status reflects the most recent desired state of the resource.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}ObservedState is the state of the resource as most recently observed in GCP.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.createTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. Creation time.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Default configuration to use for requests with this Recognizer. This can be overwritten by inline configuration in the [RecognizeRequest.config][google.cloud.speech.v2.RecognizeRequest.config] field.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}Speech adaptation context that weights recognizer predictions for specific words and phrases.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of inline CustomClasses. Existing CustomClass resources can be referenced directly in a PhraseSet.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].createTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. Creation time.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].deleteTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this resource was requested for deletion.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].etag</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. This checksum is computed by the server based on the value of other fields. This may be sent on update, undelete, and delete requests to ensure the client has an up-to-date value before proceeding.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].expireTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this resource will be purged.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].kmsKeyName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which the CustomClass is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].kmsKeyVersionName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key version name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions) with which the CustomClass is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].name</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. Identifier. The resource name of the CustomClass. Format: `projects/{project}/locations/{location}/customClasses/{custom_class}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].reconciling</code></td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Output only. Whether or not this CustomClass is in the process of being updated.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].state</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The CustomClass lifecycle state.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].uid</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. System-assigned unique identifier for the CustomClass.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.customClasses[].updateTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The most recent time this resource was modified.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets</code></td>
        <td>
            <p><code class="apitype">list (object)</code></p>
            <p>{% verbatim %}A list of inline or referenced PhraseSets.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[]</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet</code></td>
        <td>
            <p><code class="apitype">object</code></p>
            <p>{% verbatim %}An inline defined PhraseSet.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.createTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. Creation time.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.deleteTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this resource was requested for deletion.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.etag</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. This checksum is computed by the server based on the value of other fields. This may be sent on update, undelete, and delete requests to ensure the client has an up-to-date value before proceeding.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.expireTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this resource will be purged.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.kmsKeyName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which the PhraseSet is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.kmsKeyVersionName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key version name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions) with which the PhraseSet is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.name</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. Identifier. The resource name of the PhraseSet. Format: `projects/{project}/locations/{location}/phraseSets/{phrase_set}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.reconciling</code></td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Output only. Whether or not this PhraseSet is in the process of being updated.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.state</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The PhraseSet lifecycle state.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.uid</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. System-assigned unique identifier for the PhraseSet.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.defaultRecognitionConfig.adaptation.phraseSets[].inlinePhraseSet.updateTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The most recent time this resource was modified.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.deleteTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this Recognizer was requested for deletion.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.etag</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. This checksum is computed by the server based on the value of other fields. This may be sent on update, undelete, and delete requests to ensure the client has an up-to-date value before proceeding.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.expireTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The time at which this Recognizer will be purged.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.kmsKeyName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key name](https://cloud.google.com/kms/docs/resource-hierarchy#keys) with which the Recognizer is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.kmsKeyVersionName</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The [KMS key version name](https://cloud.google.com/kms/docs/resource-hierarchy#key_versions) with which the Recognizer is encrypted. The expected format is `projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{crypto_key_version}`.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.reconciling</code></td>
        <td>
            <p><code class="apitype">boolean</code></p>
            <p>{% verbatim %}Output only. Whether or not this Recognizer is in the process of being updated.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.state</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The Recognizer lifecycle state.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.uid</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. System-assigned unique identifier for the Recognizer.{% endverbatim %}</p>
        </td>
    </tr>
    <tr>
        <td><code>observedState.updateTime</code></td>
        <td>
            <p><code class="apitype">string</code></p>
            <p>{% verbatim %}Output only. The most recent time this Recognizer was modified.{% endverbatim %}</p>
        </td>
    </tr>
</tbody>
</table>

## Sample YAML(s)


Note: If you have any trouble with instantiating the resource, refer to <a href="/config-connector/docs/troubleshooting">Troubleshoot Config Connector</a>.

{% endblock %}